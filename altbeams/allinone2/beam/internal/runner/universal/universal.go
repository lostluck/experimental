package universal

import (
	"context"
	"fmt"
	"io"
	"log/slog"
	"os"
	"strings"
	"sync/atomic"
	"time"

	"github.com/lostluck/experimental/altbeams/allinone2/beam/coders"
	"github.com/lostluck/experimental/altbeams/allinone2/beam/internal/beamopts"
	"github.com/lostluck/experimental/altbeams/allinone2/beam/internal/harness"
	jobpb "github.com/lostluck/experimental/altbeams/allinone2/beam/internal/model/jobmanagement_v1"
	pipepb "github.com/lostluck/experimental/altbeams/allinone2/beam/internal/model/pipeline_v1"
	"github.com/pkg/errors"
	"google.golang.org/grpc"
	"google.golang.org/grpc/metadata"
	"google.golang.org/protobuf/encoding/prototext"
	"google.golang.org/protobuf/encoding/protowire"
	"google.golang.org/protobuf/proto"
)

var unique int32

// GetJobName returns the specified job name or, if not present, a fresh
// autogenerated name. Convenience function.
func getJobName(opts beamopts.Struct) string {
	if opts.Name == "" {
		id := atomic.AddInt32(&unique, 1)
		return fmt.Sprintf("go-job-%v-%v", id, time.Now().UnixNano())
	}
	return opts.Name
}

const idKey = "worker_id"

// writeWorkerID write the worker ID to an outgoing gRPC request context. It
// merges the information with any existing gRPC metadata.
func writeWorkerID(ctx context.Context, id string) context.Context {
	md := metadata.New(map[string]string{
		idKey: id,
	})
	if old, ok := metadata.FromOutgoingContext(ctx); ok {
		md = metadata.Join(md, old)
	}
	return metadata.NewOutgoingContext(ctx, md)
}

func Execute(ctx context.Context, p *pipepb.Pipeline, opts beamopts.Struct) (*Pipeline, error) {
	cc, err := harness.Dial(ctx, opts.Endpoint, 2*time.Minute)
	if err != nil {
		return nil, fmt.Errorf("connecting to job service: %w", err)
	}
	client := jobpb.NewJobServiceClient(cc)

	prepReq := &jobpb.PrepareJobRequest{
		Pipeline:        p,
		PipelineOptions: nil,
		JobName:         getJobName(opts),
	}
	prepResp, err := client.Prepare(ctx, prepReq)
	if err != nil {
		return nil, fmt.Errorf("preparing job name %v: %w", prepReq.JobName, err)
	}

	// (2) Stage artifacts.
	ctx = writeWorkerID(ctx, prepResp.GetPreparationId())
	artcc, err := harness.Dial(ctx,
		prepResp.GetArtifactStagingEndpoint().GetUrl(), 2*time.Minute)
	if err != nil {
		return nil, fmt.Errorf("connecting to artifact service: %w", err)
	}
	defer artcc.Close()
	if err := stageViaPortableAPI(ctx, artcc, prepResp.GetStagingSessionToken()); err != nil {
		return nil, fmt.Errorf("staging artifacts: %w", err)
	}

	// (3) Submit job
	runReq := &jobpb.RunJobRequest{
		PreparationId:  prepResp.GetPreparationId(),
		RetrievalToken: prepResp.GetStagingSessionToken(),
	}

	runResp, err := client.Run(ctx, runReq)
	if err != nil {
		return nil, fmt.Errorf("failed to submit job: %w", err)
	}

	handle := &Pipeline{
		pipe:   p,
		jobID:  runResp.GetJobId(),
		client: client,
		close: func() {
			cc.Close()
		},
		// TODO make logger configurable.
		logger: slog.New(disabledHandler{}),
	}

	return handle, handle.Wait(ctx)
}

type disabledHandler struct {
	slog.Handler
}

func (disabledHandler) Enabled(ctx context.Context, level slog.Level) bool {
	return false
}

// waitForCompletion monitors the given job until completion. It logs any messages
// and state changes received.
func waitForCompletion(ctx context.Context, logger *slog.Logger, client jobpb.JobServiceClient, jobID string) error {
	stream, err := client.GetMessageStream(ctx, &jobpb.JobMessagesRequest{JobId: jobID})
	if err != nil {
		return errors.Wrap(err, "failed to get job stream")
	}

	mostRecentError := "<no error received>"
	var errReceived, jobFailed bool

	for {
		msg, err := stream.Recv()
		if err != nil {
			if err == io.EOF {
				if jobFailed {
					// Connection finished, so time to exit, produce what we have.
					return errors.Errorf("job %v failed:\n%v", jobID, mostRecentError)
				}
				return nil
			}
			return fmt.Errorf("message stream ended for job %q: %w", jobID, err)
		}

		switch {
		case msg.GetStateResponse() != nil:
			resp := msg.GetStateResponse()

			//	slog.InfoContext(ctx, "Job state update", "job_id", jobID, "state", resp.GetState().String())

			switch resp.State {
			case jobpb.JobState_DONE, jobpb.JobState_CANCELLED:
				return nil
			case jobpb.JobState_FAILED:
				jobFailed = true
				if errReceived {
					return errors.Errorf("job %v failed:\n%v", jobID, mostRecentError)
				}
				// Otherwise we should wait for at least one error log from the runner.
			}

		case msg.GetMessageResponse() != nil:
			resp := msg.GetMessageResponse()

			text := fmt.Sprintf("%v (%v): %v", resp.GetTime(), resp.GetMessageId(), resp.GetMessageText())
			logger.Log(ctx, messageSeverity(resp.GetImportance()), text)

			if resp.GetImportance() >= jobpb.JobMessage_JOB_MESSAGE_ERROR {
				errReceived = true
				mostRecentError = resp.GetMessageText()

				if jobFailed {
					return fmt.Errorf("job %v failed:\n%w", jobID, errors.New(mostRecentError))
				}
			}

		default:
			return fmt.Errorf("unexpected job update: %v", prototext.Format(msg))
		}
	}
}

func messageSeverity(importance jobpb.JobMessage_MessageImportance) slog.Level {
	switch importance {
	case jobpb.JobMessage_JOB_MESSAGE_ERROR:
		return slog.LevelError
	case jobpb.JobMessage_JOB_MESSAGE_WARNING:
		return slog.LevelWarn
	case jobpb.JobMessage_JOB_MESSAGE_BASIC:
		return slog.LevelInfo
	case jobpb.JobMessage_JOB_MESSAGE_DEBUG, jobpb.JobMessage_JOB_MESSAGE_DETAILED:
		return slog.LevelDebug
	default:
		return slog.LevelInfo
	}
}

type Pipeline struct {
	pipe *pipepb.Pipeline

	jobID  string
	client jobpb.JobServiceClient
	logger *slog.Logger

	close func()
}

func (p *Pipeline) Wait(ctx context.Context) error {
	return waitForCompletion(ctx, p.logger, p.client, p.jobID)
}

func (p *Pipeline) Cancel(ctx context.Context) (jobpb.JobState_Enum, error) {
	request := &jobpb.CancelJobRequest{JobId: p.jobID}
	response, err := p.client.Cancel(ctx, request)
	if err != nil {
		return jobpb.JobState_UNSPECIFIED, errors.Wrapf(err, "failed to cancel job: %v", p.jobID)
	}
	return response.GetState(), nil
}

func (p *Pipeline) Metrics(ctx context.Context) (*Results, error) {
	// TODO cache on job completion.
	request := &jobpb.GetJobMetricsRequest{JobId: p.jobID}
	response, err := p.client.GetJobMetrics(ctx, request)
	if err != nil {
		return nil, errors.Wrapf(err, "failed to get metrics for : %v", p.jobID)
	}
	return &Results{pipe: p.pipe, res: response.GetMetrics()}, nil
}

type Results struct {
	pipe *pipepb.Pipeline
	res  *jobpb.MetricResults
}

func (r *Results) UserCounters() map[string]int64 {
	cs := map[string]int64{}
	for _, mon := range r.res.GetCommitted() {
		// TODO also report PCollection metrics.
		if mon.Urn != "beam:metric:user:sum_int64:v1" {
			continue
		}
		if mon.Type != "beam:metrics:sum_int64:v1" {
			continue
		}
		un := r.pipe.GetComponents().GetTransforms()[mon.Labels["PTRANSFORM"]].GetUniqueName()

		key := fmt.Sprintf("%s.%s", un, mon.Labels["NAME"])
		v, _ := protowire.ConsumeVarint(mon.GetPayload())
		cs[key] = int64(v)
	}
	return cs
}

func (r *Results) UserDistributions() map[string]struct{ Count, Sum, Min, Max int64 } {
	cs := map[string]struct{ Count, Sum, Min, Max int64 }{}
	for _, mon := range r.res.GetCommitted() {
		if mon.Urn != "beam:metric:user:distribution_int64:v1" {
			continue
		}
		if mon.Type != "beam:metrics:distribution_int64:v1" {
			continue
		}
		un := r.pipe.GetComponents().GetTransforms()[mon.Labels["PTRANSFORM"]].GetUniqueName()

		key := fmt.Sprintf("%s.%s", un, mon.Labels["NAME"])
		dec := coders.NewDecoder(mon.GetPayload())
		count := int64(dec.Varint())
		sum := int64(dec.Varint())
		min := int64(dec.Varint())
		max := int64(dec.Varint())
		cs[key] = struct{ Count, Sum, Min, Max int64 }{
			Count: count,
			Sum:   sum,
			Min:   min,
			Max:   max,
		}
	}
	return cs
}

func (r *Results) Committed(name string) int64 {
	for _, mon := range r.res.GetCommitted() {
		key := fmt.Sprintf("%s.%s", mon.Labels["PTRANSFORM"], mon.Labels["NAME"])
		if key != name {
			continue
		}
		v, _ := protowire.ConsumeVarint(mon.GetPayload())
		return int64(v)
	}
	return 0
}

// stageViaPortableAPI is a beam internal function for uploading artifacts to the staging service
// via the portable API.
//
// It will be unexported at a later time.
func stageViaPortableAPI(ctx context.Context, cc *grpc.ClientConn, st string) error {
	const attempts = 3
	var failures []string
	for {
		err := stageFiles(ctx, cc, st)
		if err == nil {
			return nil // success!
		}
		failures = append(failures, err.Error())
		if len(failures) > attempts {
			return errors.Errorf("failed to stage artifacts for token %v in %v attempts: %v", st, attempts, strings.Join(failures, ";\n"))
		}
	}
}

func stageFiles(ctx context.Context, cc *grpc.ClientConn, st string) error {
	client := jobpb.NewArtifactStagingServiceClient(cc)
	stream, err := client.ReverseArtifactRetrievalService(ctx)
	if err != nil {
		return err
	}
	defer func() {
		if err := stream.CloseSend(); err != nil {
			slog.ErrorContext(ctx, "StageViaPortableApi CloseSend", "error", err)
		}
	}()

	if err := stream.Send(&jobpb.ArtifactResponseWrapper{StagingToken: st}); err != nil {
		return errors.Wrapf(err, "failed to send staging token")
	}

	for {
		in, err := stream.Recv()
		if err == io.EOF {
			return nil
		}
		if err != nil {
			return err
		}

		slog.InfoContext(ctx, "reverse artifact request", "req", prototext.Format(in))

		switch request := in.Request.(type) {
		case *jobpb.ArtifactRequestWrapper_ResolveArtifact:
			err = stream.Send(&jobpb.ArtifactResponseWrapper{
				Response: &jobpb.ArtifactResponseWrapper_ResolveArtifactResponse{
					ResolveArtifactResponse: &jobpb.ResolveArtifactsResponse{
						Replacements: request.ResolveArtifact.Artifacts,
					},
				}})
			if err != nil {
				return err
			}

		case *jobpb.ArtifactRequestWrapper_GetArtifact:
			switch typeUrn := request.GetArtifact.Artifact.TypeUrn; typeUrn {
			case "beam:artifact:type:file:v1":
				typePl := pipepb.ArtifactFilePayload{}
				if err := proto.Unmarshal(request.GetArtifact.Artifact.TypePayload, &typePl); err != nil {
					return errors.Wrap(err, "failed to parse artifact file payload")
				}
				if err := stageFile(typePl.GetPath(), stream); err != nil {
					if err == io.EOF {
						continue // so we can get the real error from stream.Recv.
					}
					return errors.Wrapf(err, "failed to stage file %v", typePl.GetPath())

				}
			default:
				return errors.Errorf("request has unexpected artifact type %s", typeUrn)
			}

		default:
			return errors.Errorf("request has unexpected type %T", request)
		}
	}
}

func stageFile(filename string, stream jobpb.ArtifactStagingService_ReverseArtifactRetrievalServiceClient) error {
	fd, err := os.Open(filename)
	if err != nil {
		return errors.Wrapf(err, "unable to open file %v", filename)
	}
	defer fd.Close()

	data := make([]byte, 1<<20)
	for {
		n, err := fd.Read(data)
		if n > 0 {
			sendErr := stream.Send(&jobpb.ArtifactResponseWrapper{
				Response: &jobpb.ArtifactResponseWrapper_GetArtifactResponse{
					GetArtifactResponse: &jobpb.GetArtifactResponse{
						Data: data[:n],
					},
				}})
			if sendErr == io.EOF {
				return sendErr
			}

			if sendErr != nil {
				return errors.Wrap(sendErr, "StageFile chunk send failed")
			}
		}

		if err == io.EOF {
			sendErr := stream.Send(&jobpb.ArtifactResponseWrapper{
				IsLast: true,
				Response: &jobpb.ArtifactResponseWrapper_GetArtifactResponse{
					GetArtifactResponse: &jobpb.GetArtifactResponse{},
				}})
			return sendErr
		}

		if err != nil {
			return err
		}
	}
}
